---
title: "09-starter-code"
format: docx
editor: visual
---

## Load Libraries

```{r}
# Load tidyverse and tidymodels packages

```

## Example: Simulate data with multiple explanatory variables

Imagine we are simulating *product-level* data for a given week, including the total sales for the product and various marketing and product-specific variables.

```{r}
# Set variable and parameter values.
nobs <- 500
intercept <- 25
# coefficient for money spent on discounts.
beta_discount <- 2
# coefficient for money spent on endcap displays
beta_endcaps <- 7
# coefficient for cold temperature product
beta_temp <- 15
# coefficient for effect of sending coupons
beta_coupon <- 5
# coefficient for effect of running an ad
beta_ad <- 10
```

```{r}
# Set the randomization seed.
set.seed(123)

sim_data <- tibble(
  # Simulate money spent on discounts.
  discount = runif(nobs, min = 0, max = 75),
  # Simulate money spent on endcap displays conditioned on cold temp.
  temp = rbinom(nobs, size = 1, prob = 0.4),
  endcaps = temp * 75 + runif(nobs, min = 0, max = 50),
  # Simulate three levels of features: none, coupon, and ads.
  feature = rbinom(nobs, size = 2, prob = 0.5),
  feature_coupon = if_else(feature == 1, 1, 0),
  feature_ad = if_else(feature == 2, 1, 0),
  # Simulate category promotion as a function of discounts and features.
  category = discount + 30 * feature + rnorm(nobs, mean = 50, sd = 5),
  # Simulate sales as a function of all four variables.
  sales = intercept + beta_discount * discount + beta_endcaps * endcaps + 
    beta_temp * temp + beta_coupon * feature_coupon + beta_ad * feature_ad + 
    rnorm(nobs, mean = 0, sd = 5)
)
```

View the simulated data.

```{r}
sim_data
```

## Fit Model with Multiple Explanatory Variables

```{r}
# Fit the complete model.
fit_01 <- linear_reg() |> 
  set_engine("lm") |> 
  fit(sales ~ discount + endcaps + temp + feature_coupon + feature_ad, data = sim_data)
```

Check whether the model recovered the correct parameter values.

```{r}
tidy(fit_01, conf.int = TRUE)
```

We can visualize our parameter estimates and confidence intervals using the plot below.

```{r}
tidy(fit_01, conf.int = TRUE) |> 
  ggplot(aes(x = term)) + 
  geom_point(aes(y = estimate)) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +
  geom_hline(yintercept = 0, color = "red")
```

## Examine Omitted Variable Bias

Sometimes our model might be missing an important part of the story. In this case, we omit the `temp` variable from our model, even though we know it is part of the story.

```{r}
# Fit a model with an omitted variable.
fit_02 <- linear_reg() |> 
  set_engine("lm") |> 
  fit(sales ~ discount + endcaps + feature_coupon + feature_ad, data = sim_data)
```

## Your Turn!

Remember that `intercept = 25`, `discount = 2`, `endcaps = 7`, `temp = 15`, `feature_coupon = 5`, and `feature_ad = 10`.

Determine whether our model recovered the correct parameter values. Why or why not?

```{r}

```

Plot the parameter estimates for the model with omitted variable bias.

```{r}
tidy(fit_02, conf.int = TRUE) |> 
  ggplot(aes(x = term)) + 
  geom_point(aes(y = estimate)) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +
  geom_hline(yintercept = 0, color = "red")
```

## What if We Include Variables We Shouldn't?

Let's run a model where we include a redundant variable, `category`, and see what happens to our parameter estimates. Look back at the code used to generate the data. How did we create `category`?

```{r}
# Fit a model with included variable bias.
fit_03 <- linear_reg() |> 
  set_engine("lm") |> 
  fit(
    sales ~ discount + endcaps + temp + feature_coupon + feature_ad + category, 
    data = sim_data
  )
```

Did we recover the correct parameter values? Remember that `intercept = 25`, `discount = 2`, `endcaps = 7`, `temp = 15`, `feature_coupon = 5`, and `feature_ad = 10`.

```{r}
tidy(fit_03, conf.int = TRUE)
```

Plot the parameter estimates.

```{r}
tidy(fit_03, conf.int = TRUE) |> 
  ggplot(aes(x = term)) + 
  geom_point(aes(y = estimate)) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +
  geom_hline(yintercept = 0, color = "red")
```

This is called **multicollinearity**, which means that two or more of our explanatory variables are *very highly correlated*.

Examine a correlation matrix. What do we notice about the correlations between `category` and the other variables?

```{r}
cor(sim_data)
```

Remember... strong correlations between our outcome variable and our explanatory variables are good...but very strong correlations between our explanatory variables are bad for inference and can prevent us from identifying correct parameter estimates.

Statistical significance and model comparisons are no substitute for careful consideration of the story behind the data.

What do you notice when you compare these three models? Is one better than the others in terms of R-squared?

```{r}
bind_rows(
  glance(fit_01), # The complete model.
  glance(fit_02), # Model with omitted variable bias.
  glance(fit_03)  # Model with collinearity
)
```
