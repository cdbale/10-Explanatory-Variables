---
title: "Explanatory Variables"
subtitle: "MCB Consulting Group (MKTG 411)"
output: 
  ioslides_presentation:
    widescreen: true
    css: style.css
---

## Marketing Analytics Process

<center>
![](Figures/process_model.png){width="900px"}
</center>

## Inferential Modeling Workflow

<center>
![](Figures/workflow-inference_build.png){width=900px}
</center>

---

![](Figures/hex_parsnip-broom.png){width=850px}

## Review the Workflow

<center>
![](Figures/workflow-inference.png){width=900px}
</center>

## Statistical and Causal Inference

<center>

![](Figures/ice-cream-shark-attacks.png){width=700px}

<center>

## Motivating Example

Kroger decided to run a promotional campaign offering targeted %-discount coupons to customers who shop at their stores nationwide. The coupons were delivered via retailer-specific apps and were valid only for the November 21st weekend. The marketing team has shared the results with us: they observed, on average, 23% higher soup category sales when the campaign was running compared to the previous weekend. The marketing team was quick to label the campaign as a success, but upper-level management is skeptical and has asked us to review and confirm the results.

## Motivating Example

Kroger decided to run a promotional campaign offering targeted %-discount coupons to customers who shop at their stores nationwide. The coupons were delivered via retailer-specific apps and were valid only for the November 21st weekend. The marketing team has shared the results with us: they observed, on average, 23% higher soup category sales when the campaign was running compared to the previous weekend. The marketing team was quick to label the campaign as a success, but upper-level management is skeptical and has asked us to review and confirm the results.

Let's come up with a story...

## Motivating Example

Kroger decided to run a promotional campaign offering targeted %-discount coupons to customers who shop at their stores nationwide. The coupons were delivered via retailer-specific apps and were valid only for the November 21st weekend. The marketing team has shared the results with us: they observed, on average, 23% higher soup category sales when the campaign was running compared to the previous weekend. The marketing team was quick to label the campaign as a success, but upper-level management is skeptical and has asked us to review and confirm the results.

What kind of data would we need to collect to truly determine if the campaign worked?

## Statistical and Causal Inference

Regression and how we interpret the parameter estimates *sounds* like causality, but regression doesn't *necessarily* describe causal relationships. There are two types of inference that are interconnected.

- **Statistical inference** is inferring associations (e.g., correlations) between variables.
- **Causal inference** is inferring *causal* associations between variables.

Discovering associations between variables is *common*. Uncovering causality is *uncommon*. We don't automatically get causality by fitting and evaluating a statistical model. We can come **close** to causality by coming up with a more realistic story...

## Realistic Stories Often Require Multiple Variables

Including more than one explanatory variable almost always gives us a more realistic representation of how our data was created (our story).

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \epsilon, \text{ where } \epsilon \sim Normal(0, 1)$$

- $y$ is the *outcome* variable.
- $x_1$ is the 1st *explanatory* variable.
- $x_p$ is the $p$th *explanatory* variable.
- $\beta_0$ is the *intercept* parameter.
- $\beta_1$ is the 1st *slope* parameter.
- $\beta_p$ is the $p$th *slope* parameter.
- $\epsilon$ is the *error* term.

## For Example...

Remember we can pretend that our model *is* the true story and use it to simulate data. 

Imagine we are simulating *product-level* data for a given week, including the total sales for the product and various marketing and product-specific variables.

---

```{r message=FALSE}
# Load packages.
library(tidyverse)
library(tidymodels)

# Set variable and parameter values.
nobs <- 500
intercept <- 25
# coefficient for money spent on discounts.
beta_discount <- 2
# coefficient for money spent on endcap displays
beta_endcaps <- 7
# coefficient for cold temperature product
beta_temp <- 15
# coefficient for effect of sending coupons
beta_coupon <- 5
# coefficient for effect of running an ad
beta_ad <- 10
```

---

```{r}
# Set the randomization seed.
set.seed(123)

sim_data <- tibble(
  # Simulate money spent on discounts.
  discount = runif(nobs, min = 0, max = 75),
  # Simulate money spent on endcap displays conditioned on cold temp.
  temp = rbinom(nobs, size = 1, prob = 0.4),
  endcaps = temp * 75 + runif(nobs, min = 0, max = 50),
  # Simulate three levels of features: none, coupon, and ads.
  feature = rbinom(nobs, size = 2, prob = 0.5),
  feature_coupon = if_else(feature == 1, 1, 0),
  feature_ad = if_else(feature == 2, 1, 0),
  # Simulate category promotion as a function of discounts and features.
  category = discount + 30 * feature + rnorm(nobs, mean = 50, sd = 5),
  # Simulate sales as a function of all four variables.
  sales = intercept + beta_discount * discount + beta_endcaps * endcaps + 
    beta_temp * temp + beta_coupon * feature_coupon + beta_ad * feature_ad + 
    rnorm(nobs, mean = 0, sd = 5)
)
```

---

```{r}
sim_data
```

## Finding the Best *Hyperplane*

We still estimate $\beta_0$ , $\beta_1$ , ..., $\beta_p$ by minimizing the sum of the squared residuals.

However, as we move from one explanatory variable (a line) to two (a plane) to three or more (a hyperplane), it becomes increasingly difficult to visualize the process of fitting the model to the data.

Fortunately, this doesn't hinder the intuition or our ability to fit multiple regression.

```{r}
# Fit the complete model.
fit_01 <- linear_reg() |> 
  set_engine("lm") |> 
  fit(sales ~ discount + endcaps + temp + feature_coupon + feature_ad, data = sim_data)
```

---

<center>

![](Figures/hyperplane.png){width=800px}

<center>

## Recover Parameters

Remember that `intercept = 25`, `discount = 2`, `endcaps = 7`, `temp = 15`, `feature_coupon = 5`, and `feature_ad = 10`.

```{r}
tidy(fit_01, conf.int = TRUE)
```

## Parameter Estimates

$$sales = 24.85 + 2.02 \times discount + 6.99 \times endcaps + \\ 15.88 \times temp + 4.65 \times feature_{coupon} + 10.01 \times feature_{ad}$$

How does this change parameter interpretations?

- The intercept parameter $\beta_0$ represents the expected value of $y$ when all explanatory variables ($x_1$, ..., $x_p$) are equal to zero.
- If $x_1$ is continuous, the associated slope parameter $\beta_1$ represents the expected amount by which $y$ will change given a one unit increase in $x_1$, **holding all other variables ($x_2$, ..., $x_p$) fixed**.
- If $x_1$ is discrete, the associated slope parameter $\beta_1$ represents the expected amount by which $y$ will change **relative** to the *baseline level* of $x_1$, **holding all other variables ($x_2$, ..., $x_p$) fixed**.

---

```{r}
tidy(fit_01, conf.int = TRUE) |> 
  ggplot(aes(x = term)) + 
  geom_point(aes(y = estimate)) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +
  geom_hline(yintercept = 0, color = "red")
```

## Omitted Variable Bias

When adding explanatory variables, we need to do everything we can to include variables that explain the outcome so we don't create **omitted variable bias**.

Why would omitting an explanatory variable create bias?

```{r}
# Fit a model with an omitted variable.
fit_02 <- linear_reg() |> 
  set_engine("lm") |> 
  fit(sales ~ discount + endcaps + feature_coupon + feature_ad, data = sim_data)
```

## Your Turn!

Remember that `intercept = 25`, `discount = 2`, `endcaps = 7`, `temp = 15`, `feature_coupon = 5`, and `feature_ad = 10`.

Determine whether our model recovered the correct parameter values. Why or why not?

---

```{r, echo = FALSE}
tidy(fit_02, conf.int = TRUE)
```

---

```{r, eval = FALSE}
tidy(fit_02, conf.int = TRUE)
```

---

```{r}
tidy(fit_02, conf.int = TRUE) |> 
  ggplot(aes(x = term)) + 
  geom_point(aes(y = estimate)) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +
  geom_hline(yintercept = 0, color = "red")
```

## What's Going On?

Why was our estimate for the effect of `endcaps` on `sales` larger than it should have been?

## What if We Include Variables We Shouldn't?

Problems from omitted variable bias might suggest we should throw everything we can in the model. However, when adding explanatory variables, we need to be careful not to include *redundant* explanatory variables.

Why would including redundant variables create problems?

```{r}
# Fit a model with included variable bias.
fit_03 <- linear_reg() |> 
  set_engine("lm") |> 
  fit(
    sales ~ discount + endcaps + temp + feature_coupon + feature_ad + category, 
    data = sim_data
  )
```

---

Remember that `intercept = 25`, `discount = 2`, `endcaps = 7`, `temp = 15`, `feature_coupon = 5`, and `feature_ad = 10`.

```{r}
tidy(fit_03, conf.int = TRUE)
```

---

```{r}
tidy(fit_03, conf.int = TRUE) |> 
  ggplot(aes(x = term)) + 
  geom_point(aes(y = estimate)) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +
  geom_hline(yintercept = 0, color = "red")
```

## Multicollinearity

More formally, when adding explanatory variables, we need to avoid including variables that are highly correlated, something called **multicollinearity**.

The intuition is that our model can't accurately determine how different explanatory variables contribute to the story if they *make very similar contributions*.

## Uncovering Multicollinearity

Let's examine a correlation matrix of our data.

## Considerations

Be thoughtful when adding explanatory variables. ALWAYS come up with a plausible story before training your model.

- Think carefully about the variables in your data! People often encounter problems when one variable is a *function* of another.
- Use `cor()` on continuous explanatory variables to check for multicollinearity.
- Statistical significance and model comparisons can be informative, but **they don't guarantee correct modeling decisions.**

---

```{r eval=FALSE}
bind_rows(
  glance(fit_01), # The complete model.
  glance(fit_02), # Model with omitted variable bias.
  glance(fit_03)  # Model with collinearity
)
```

```{r echo=FALSE}
bind_rows(
  glance(fit_01), # The complete model.
  glance(fit_02), # Model with omitted variable bias.
  glance(fit_03)  # Model with collinearity
) |> 
  select(r.squared:logLik) |> 
  as.data.frame()
```

## Don't be a Cuttlefish!

<center>
![](Figures/cuttlefish.jpg){width="900px"}
</center>

## Have Integrity

"People can behave like cuttlefish, changing their clothing, language, behavior, and thoughts, depending on their surroundings. They might do this in order to impress or be accepted by others. However, this behavior does people more harm than good. In their eagerness to blend in, they can sacrifice their integrity."

[Barbara A. Lewis](https://www.churchofjesuschrist.org/study/ensign/2018/04/integrity?lang=eng)

## Wells Fargo Cross-Selling Fraud

- Internal Pressure: We *have* to meet sales goals!
- Decision: employees opened millions of unauthorized accounts.
- Result: [Wells Fargo agrees to pay $3 billion in settlement](https://www.justice.gov/archives/opa/pr/wells-fargo-agrees-pay-3-billion-resolve-criminal-and-civil-investigations-sales-practices)

## Purdue Pharma Opioid Scandal

- Internal Pressure: make money, sell prescription drugs.
- Decision: Aggressive marketing of opioid drugs, including downplaying the addictive risks and encouraging over-prescription.
- Result: [Criminal guilty pleas, $8 billion settlement, dissolution of the company and repurposed for public benefit.](https://www.justice.gov/archives/opa/pr/justice-department-announces-global-resolution-criminal-and-civil-investigations-opioid).

## What Should I Do?

- Be transparent about the decisions you make and your motivations for those decisions. Don't hunt for statistical significance!
- Be honest about uncertainty. Are there any variables that are not included in your model that could bias your results?
- Conduct analyses *in the pursuit of truth*, NOT in the pursuit of a specific result.

## Wrapping Up

*Summary*

- Extended regression to include multiple explanatory variables.
- Introduce plotting for parameter estimates.
- Used simulated data to demonstrate challenges with adding explanatory variables.

*Next Time*

- Overfitting and prediction.

## Exercise 9

Multiple regression is often referred to as "key drivers analysis" in business applications. Kroger has requested a key drivers analysis for `Sales` of `CAMPBELL'S` brand products in the `WEST CENSUS TA` Retailer Trade Area.

- Build at least two different regression models for `Sales` using any combination of the variables `Any_Disp_Spend` (spend on in-store displays), `Any_Feat_Spend` (spend on coupons and other printed promotions, collectively called "features"), and `Any_Price_Decr_Spend` (spend on price decreases).
- Compare each of the models' R-squared. Which one explains the most variation in `Sales`?

Continued on next slide...

---

- What are some omitted variables that could affect the estimates of the coefficients in your model? No need to build an exhaustive list, just come up with one plausible example.
- Using the best fitting model from part 2, predict counterfactual `Sales` for Campbellâ€™s. Assume that Kroger can spend up to $10,000 for this trading area. Produce predictions for three counterfactual scenarios and use them to detail a proposal for how Kroger should promote Campbell's products for the `WEST CENSUS TA`.
- Render the Quarto document into Word and upload to Canvas.

